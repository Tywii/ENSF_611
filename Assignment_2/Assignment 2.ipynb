{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4600, 57) (4600,) <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets import load_spam\n",
    "X, y = load_spam()\n",
    "print(X.shape, y.shape, type(X), type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "X.isna().values.any()\n",
    "y.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "size = int(len(X) * 0.05)\n",
    "X_small = X.sample(n=size)\n",
    "y_small = y.sample(n=size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2992cb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lr_full = LogisticRegression(max_iter=2000)\n",
    "lr_full.fit(X_train, y_train)\n",
    "\n",
    "X_first_two_train, X_first_two_test, y_first_two_train, y_first_two_test = train_test_split(X.iloc[:,:2], y)\n",
    "lr_first_two = LogisticRegression(max_iter=2000)\n",
    "lr_first_two.fit(X_first_two_train, y_first_two_train)\n",
    "\n",
    "X_small_train, X_small_test, y_small_train, y_small_test = train_test_split(X_small, y_small)\n",
    "lr_small = LogisticRegression(max_iter=2000)\n",
    "lr_small.fit(X_small_train, y_small_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77c19ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full set training set score: 0.930\n",
      "Full set validation set score: 0.935\n",
      "\n",
      "First two rows training set score: 0.606\n",
      "First two rows validation set score: 0.619\n",
      "\n",
      "Five percent set training set score: 0.773\n",
      "Five percent set validation set score: 0.569\n"
     ]
    }
   ],
   "source": [
    "print(\"Full set training set score: {:.3f}\".format(lr_full.score(X_train, y_train)))\n",
    "print(\"Full set validation set score: {:.3f}\\n\".format(lr_full.score(X_test, y_test)))\n",
    "\n",
    "print(\"First two rows training set score: {:.3f}\".format(lr_first_two.score(X_first_two_train, y_first_two_train)))\n",
    "print(\"First two rows validation set score: {:.3f}\\n\".format(lr_first_two.score(X_first_two_test, y_first_two_test)))\n",
    "\n",
    "print(\"Five percent set training set score: {:.3f}\".format(lr_small.score(X_small_train, y_small_train)))\n",
    "print(\"Five percent set validation set score: {:.3f}\".format(lr_small.score(X_small_test, y_small_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data size  training accuracy  validation accuracy\n",
      "0  (4600, 57)           0.929565             0.934783\n",
      "1   (4600, 2)           0.606087             0.619130\n",
      "2   (230, 57)           0.773256             0.568966\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE TH E DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "results = pd.DataFrame(columns=['data size', 'training accuracy', 'validation accuracy'])\n",
    "results.loc[len(results)] = [X.shape, lr_full.score(X_train, y_train), lr_full.score(X_test, y_test)]\n",
    "results.loc[len(results)] = [X.iloc[:,:2].shape, lr_first_two.score(X_first_two_train, y_first_two_train), lr_first_two.score(X_first_two_test, y_first_two_test)]\n",
    "results.loc[len(results)] = [X_small.shape, lr_small.score(X_small_train, y_small_train), lr_small.score(X_small_test, y_small_test)]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. Training and validation accuracy increase with more data used. Using five percent of the original dataset resulted in a reduction in training accuracy from 0.93 to 0.76, and a reduction in validation accuracy from 0.92 to 0.53. Reducing the amount of columns from 57 to 2 resulted in a reduction in training accuracy from 0.93 to 0.61, and a reduction in validation accuracy from 0.92 to 0.64.\n",
    "\n",
    "2. A false positive would be a non-spam email marked as spam. A false negative would be a spam email marked as not spam. A false positive would be worse (marking non-spam email as spam could result in email being missed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0a639",
   "metadata": {},
   "source": [
    "1. Code was sourced from the course examples (Linear Regression-filled.ipynb, Linnear Classification-filled.ipynb), and from the following websites:\n",
    "    https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "2. The steps were completed in order.\n",
    "\n",
    "3. Generative AI was not used. I find I learn better researching my questions rather than asking a generative AI program. \n",
    "\n",
    "4. No major challenges. Just remembering some syntax, functions to use etc. Going over the course examples helped a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 8) (1030,) <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()\n",
    "print(X.shape, y.shape, type(X), type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "X.isna().values.any()\n",
    "y.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "970c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "R2_training = model.score(X_train, y_train)\n",
    "mse_training = mean_squared_error(y_train, model.predict(X_train))\n",
    "R2_validation = model.score(X_test, y_test)\n",
    "mse_validation = mean_squared_error(y_test, model.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Training accuracy Validation accuracy\n",
      "MSE             110.660379           98.297955\n",
      "R2 score          0.597044            0.661224\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "results = pd.DataFrame(columns=['Training accuracy', 'Validation accuracy'], index=['MSE', 'R2 score'])\n",
    "\n",
    "results.loc['MSE', 'Training accuracy'] = mse_training\n",
    "results.loc['MSE', 'Validation accuracy'] = mse_validation\n",
    "results.loc['R2 score', 'Training accuracy'] = R2_training\n",
    "results.loc['R2 score', 'Validation accuracy'] = R2_validation\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not? \n",
    "    \n",
    "    The linear model produced inaccurate results for the dataset. This is because the data does not represent a very linear trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "1. Code was sourced from the course examples (Linear Regression-filled.ipynb, Linnear Classification-filled.ipynb), and from the following websites:<br>\n",
    "    https://www.scikit-yb.org/en/latest/api/datasets/spam.html<br>\n",
    "    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "2. The steps were completed in order.\n",
    "\n",
    "3. Generative AI was not used. I find I learn better researching my questions rather than asking a generative AI program. \n",
    "\n",
    "4. No major challenges. Just remembering some syntax, functions to use etc. Going over the course examples helped a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*<br>\n",
    "The main pattern / finding I noticed is the importance of using the correct models for your dataset - for example using linear regression was not effective with the concrete dataset, as it does not follow a very linear trend. This resulted training and validation scores of 110.7 and 98.3 for MSE, and 0.60 and 0.66 for R2, respectively. \n",
    "\n",
    "The analysis done in Part 1 of this assignment with the spam dataset showed the importance of having sufficient data, as we saw that we could only produce good results with sufficient columns and amount of data. (Using five percent of the original dataset resulted in a reduction in training accuracy from 0.93 to 0.76, and a reduction in validation accuracy from 0.92 to 0.53. Reducing the amount of columns from 57 to 2 resulted in a reduction in training accuracy from 0.93 to 0.61, and a reduction in validation accuracy from 0.92 to 0.64)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*<br>\n",
    "I liked how the assignment broken down into steps, as it helps walk us through the assigment. I liked being able to try out different methods on different models to see how they work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5970437074602829\n",
      "0.6612241937986676\n",
      "0.001\n",
      "0.001\n",
      "0.5970437072219401\n",
      "0.661221890676573\n",
      "0.001\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "max_ridge_training = 0\n",
    "max_ridge_validation = 0\n",
    "max_alpha_ridge_training = 0\n",
    "max_alpha_ridge_validation = 0\n",
    "\n",
    "max_lasso_training = 0\n",
    "max_lasso_validation = 0\n",
    "max_alpha_lasso_training = 0\n",
    "max_alpha_lasso_validation = 0\n",
    "\n",
    "for i in np.arange(0.001, 0.05, 100):\n",
    "    ridge = Ridge(alpha=i).fit(X_train, y_train)\n",
    "    lasso = Lasso(alpha = i).fit(X_train, y_train)\n",
    "    \n",
    "    if ridge.score(X_train, y_train) > max_ridge_training:\n",
    "        max_ridge_training = ridge.score(X_train, y_train)\n",
    "        max_alpha_ridge_training = i\n",
    "    if ridge.score(X_test, y_test) > max_ridge_validation:\n",
    "        max_ridge_validation = ridge.score(X_test, y_test)\n",
    "        max_alpha_ridge_validation = i\n",
    "\n",
    "    if lasso.score(X_train, y_train) > max_lasso_training:\n",
    "        max_lasso_training = lasso.score(X_train, y_train)\n",
    "        max_alpha_lasso_training = i\n",
    "    if lasso.score(X_test, y_test) > max_lasso_validation:\n",
    "        max_lasso_validation = lasso.score(X_test, y_test)\n",
    "        max_alpha_lasso_validation = i\n",
    "\n",
    "print(max_ridge_training)\n",
    "print(max_ridge_validation)\n",
    "print(max_alpha_ridge_training)\n",
    "print(max_alpha_ridge_validation)\n",
    "\n",
    "print(max_lasso_training)\n",
    "print(max_lasso_validation)\n",
    "print(max_alpha_lasso_training)\n",
    "print(max_alpha_lasso_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*<br>\n",
    "The lowest alpha values gave the highest R2 score. Ridge regression gave the highest training and validation set scores. The validation scores, which show how well the model generalizes are low for all tested alpha values, meaning the score is not 'good enough'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('ensf-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3de7b0f19889569506ef001b2e852b5bf44bf616beb1f25127cd4bd75dca059"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
